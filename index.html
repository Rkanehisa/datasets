<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Generating Large Scale Image Datasets from 3D CAD Models by UMassLowell-Vision-Group</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Generating Large Scale Image Datasets from 3D CAD Models</h1>
      <h2 class="project-tagline"></h2>
      <a href="https://github.com/UMassLowell-Vision-Group/datasets" class="btn">View on GitHub</a>
      <a href="https://github.com/UMassLowell-Vision-Group/datasets/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/UMassLowell-Vision-Group/datasets/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="generating-large-scale-image-datasets-from-3d-cad-models" class="anchor" href="#generating-large-scale-image-datasets-from-3d-cad-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Generating Large Scale Image Datasets from 3D CAD Models</h1>

<p>Published at CVPR'15 Workshop on <a href="https://sites.google.com/site/cvpr2015futureofdataworkshop/">The Future of Datasets in Vision</a></p>

<p><a href="http://vision.cs.uml.edu/pubs/cvpr2015_workshop_virtual_dataset.pdf">Paper</a></p>

<h2>
<a id="citation" class="anchor" href="#citation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Citation</h2>

<pre><code>@inproceedings{baochen15fdv,
    Author = {Baochen Sun and Xingchao Peng and Kate Saenko},
    Title = {Generating Large Scale Image Datasets from 3D CAD Models},
    Booktitle = {CVPR Workshop},
    Year = {2015}
}
</code></pre>

<h2>
<a id="dataset-one" class="anchor" href="#dataset-one" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset One</h2>

<p><a href="http://www.cs.uml.edu/%7Ebsun/bmvc14.zip">Dataset</a></p>

<p>Dataset for "From Virtual to Reality: Fast Adaptation of Virtual Object Detectors to Real Domains" published in British Machine Vision Conference(BMVC) 2014.</p>

<p><a href="https://github.com/UMassLowell-Vision-Group/bmvc2014/raw/master/bmvc14_paper.pdf">Paper</a></p>

<p><a href="https://github.com/UMassLowell-Vision-Group/bmvc2014/raw/master/bmvc14_extended_abstract.pdf">Extended Abstract</a></p>

<p><a href="https://github.com/UMassLowell-Vision-Group/bmvc2014/raw/master/bmvc14_poster.pdf">Poster</a></p>

<h2>
<a id="citation-1" class="anchor" href="#citation-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Citation</h2>

<pre><code>@inproceedings{baochen14BMVC,
    Author = {Baochen Sun and Kate Saenko},
    Title = {From Virtual to Reality: Fast Adaptation of Virtual Object Detectors to Real Domains},
    Booktitle = {British Machine Vision Conference},
    Year = {2014}
}
</code></pre>

<h2>
<a id="dataset-two" class="anchor" href="#dataset-two" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset Two</h2>

<p><a href="http://www.cs.uml.edu/%7Expeng/ICLR2015.zip">Dataset</a></p>

<p>Dataset for "What Do Deep CNNs Learn About Object?" published in ICLR'15 workshop.</p>

<p><a href="http://arxiv.org/abs/1504.02485">Workshop Paper</a></p>

<p><a href="http://arxiv.org/abs/1412.7122">Arxiv Full Paper</a></p>

<p>Poster: TBD</p>

<h2>
<a id="citation-2" class="anchor" href="#citation-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Citation</h2>

<pre><code>@inproceedings{xpengiclr15,
  author    = {Xingchao Peng and
               Baochen Sun and
               Karim Ali and
               Kate Saenko},
  title     = {Exploring Invariances in Deep Convolutional Neural Networks Using
               Synthetic Images},
  journal   = {CoRR},
  volume    = {abs/1412.7122},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.7122},
  timestamp = {Thu, 01 Jan 2015 19:51:08 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/PengSAS14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
</code></pre>

<h2>
<a id="faq" class="anchor" href="#faq" aria-hidden="true"><span class="octicon octicon-link"></span></a>FAQ</h2>

<p>Please go to the GitHub repo (<a href="https://github.com/UMassLowell-Vision-Group/From-Virtual-to-Reality">https://github.com/UMassLowell-Vision-Group/From-Virtual-to-Reality</a>) for source code, 3d Models, datasets of Dataset One. The source code of Dataset Two is similar to Dataset One and more explainations are in the following. All the files corresponding to Dataset Two should be available soon. The following explanations are based on Dataset One.</p>

<p>The source code is available in 'code' folder. The render.ms is the file for the rendering part. All the rendered images (with annotation) are in these two folders: 'virtual' and 'virtual_gray'. The 3D models is in the '3d_models' folder. Basically, here are the brief steps:</p>

<ol>
<li><p>Download the 3D models</p></li>
<li><p>Run render.ms in 3ds Max (the software we used to generate the dataset)</p></li>
</ol>

<p>The are also limited comments in render.ms which might help you understand the code.</p>

<p>To generate more realistic images (as in the 'What Do Deep CNNs Learn About Object' paper), you may need to specify different background and texture for different category/3d model. Then you need to change the 'images_bg' and 'images_texture' in the render.ms file to point to different background and textures for different category/3d model.</p>

<p>For now, we did not implemented the full photorealistic rendering since it is more complicated (e.g. might need to use ray-tracking algorithms to do that) and take far more time (e.g. hours) to render one image. However, as showed in the 'From-Virtual-to-Reality' paper, simple domain adaptation techniques can get same performance as training classifiers with real images (e.g. images from ImageNet).</p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/UMassLowell-Vision-Group/datasets">Generating Large Scale Image Datasets from 3D CAD Models</a> is maintained by <a href="https://github.com/UMassLowell-Vision-Group">UMassLowell-Vision-Group</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
